@article{Leem_2023_3,
  author={S.-G. Leem and D. Fulford and J.-P. Onnela and D. Gard and C. Busso},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Selective Acoustic Feature Enhancement for Speech Emotion Recognition with Noisy Speech}, 
  year={2023},
  volume={to appear},
  number={},
  pages={1-14},
  doi={10.1109/TASLP.2023.3340603}
}

@inproceedings{Naini_2023_3, 
	author={A. {Reddy Naini} and S. Subramanium and S.-G. Leem and C. Busso}, 
	title={Combining relative and absolute learning formulations to predict emotional attributes from speech},
	booktitle={ IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2023)}, 
	volume={},
	year={2023}, 
	month={December}, 
	address =  {Taipei, Taiwan},
	pages={}, 
	doi={},
}

@inproceedings{Martinez-Lucas_2023, 
	author={L. Martinez-Lucas and A. Salman and S.-G. Leem and S.G. Upadhyay and C.-C. Lee and C. Busso}, 
	title={Analyzing the Effect of Affective Priming on Emotional Annotations},
	booktitle={International Conference on Affective Computing and Intelligent Interaction (ACII 2023)}, 
	volume={},
	year={2023}, 
	month={September}, 
	address =  {Cambridge, MA, USA},
	pages={}, 
	doi={},
}

@InProceedings{Leem_2023_2, 
	author={S.-G. Leem and D. Fulford and J.-P. Onnela and D. Gard and C. Busso}, 
	title={Computation and Memory Efficient Noise Adaptation of {Wav2Vec2.0} for Noisy Speech Emotion Recognition with Skip Connection Adapters},
	booktitle={Interspeech 2023}, 
	year={2023}, 
	month={August}, 
	address =  {Dublin, Ireland},
	pages={1888-1892}, 
	doi={10.21437/Interspeech.2023-1034},
}

@InProceedings{Chou_2023, 
	author={H.-C. Chou and L. Goncalves and S.-G. Leem and C.-C. Lee and C. Busso}, 
	title={The Importance of Calibration: Rethinking Confidence and Performance of Speech Multi-label Emotion Classifiers},
	booktitle={Interspeech 2023}, 
	year={2023}, 
	month={August}, 
	address =  {Dublin, Ireland},
	pages={641-645}, 
	doi={10.21437/Interspeech.2023-1113},
}

@InProceedings{Leem_2023, 
	author={S.-G. Leem and D. Fulford and J.-P. Onnela and D. Gard and C. Busso}, 
	title={Adapting a self-supervised speech representation for noisy speech emotion recognition by using contrastive teacher-student learning},
	booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023),}, 
	volume={},
	year={2023}, 
	month={}, 
	address =  {Rhodes island, Greece},
	pages={1-5}, 
	doi={10.1109/ICASSP49357.2023.10097135},
}

@InProceedings{Leem_2022, 
	author={S.-G. Leem and D. Fulford and J.-P. Onnela and D. Gard and C. Busso}, 
	title={Not All Features Are Equal: Selection of Robust Features for Speech Emotion Recognition in Noisy Environments },
	booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2022)}, 
	volume={},
	year={2022}, 
	month={May}, 
	address =  {Singapore},
	pages={6447-6451},
  doi={10.1109/ICASSP43922.2022.9747705}
}

@InProceedings{Leem_2021, 
	author={S.-G. Leem and D. Fulford and J.-P. Onnela and D. Gard and C. Busso}, 
	title={Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions},
	booktitle={Interspeech 2021}, 
	volume={},
	year={2021}, 
	month={August-September}, 
	pages={2871-2875}, 
	address =  {Brno, Czech Republic},
	doi={10.21437/Interspeech.2021-1438},
	}

@unpublished{Goncalves_2023_2,
	author = {L. Goncalves and S.-G. Leem and W.-C. Lin and B. Sisman and C. Busso},
	title = {Versatile Audiovisual Learning for Handling Single and Multi Modalities in Emotion Regression and Classification Tasks},
	journal = {ArXiv e-prints (arXiv:2305.07216 )},
	archivePrefix = {arXiv},
	eprint = {2305.07216 },
	primaryClass = {cs.LG},
	volume = {},
	number = {},
	year = {2023},
	pages = {1-14},
	month = {May},
	doi={10.48550/arXiv.2305.07216},
}

@ARTICLE{Harvill_2023,
  author={Harvill, John and Leem, Seong-Gyun and AbdelWahab, Mohammed and Lotfian, Reza and Busso, Carlos},
  journal={IEEE Transactions on Affective Computing}, 
  title={Quantifying Emotional Similarity in Speech}, 
  year={2023},
  volume={14},
  number={2},
  pages={1376-1390},
  doi={10.1109/TAFFC.2021.3127390}}

@ARTICLE{Yoo_2020,
  author={Yoo, In-Chul and Lee, Keonnyeong and Leem, Seonggyun and Oh, Hyunwoo and Ko, Bonggu and Yook, Dongsuk},
  journal={IEEE Access}, 
  title={Speaker Anonymization for Personal Information Protection Using Voice Conversion Techniques}, 
  year={2020},
  volume={8},
  number={},
  pages={198637-198645},
  doi={10.1109/ACCESS.2020.3035416}}


@inproceedings{Yook_2020,
  author={Dongsuk Yook and Seong-Gyun Leem and Keonnyeong Lee and In-Chul Yoo},
  title={{Many-to-Many Voice Conversion Using Cycle-Consistent Variational Autoencoder with Multiple Decoders}},
  year=2020,
  booktitle={Proc. The Speaker and Language Recognition Workshop (Odyssey 2020)},
  pages={215--221},
  doi={10.21437/Odyssey.2020-31}
}

@ARTICLE{Leem_2019,
  author={Leem, Seong-Gyun and Yoo, In-Chul and Yook, Dongsuk},
  journal={IEEE Transactions on Consumer Electronics}, 
  title={Multitask Learning of Deep Neural Network-Based Keyword Spotting for IoT Devices}, 
  year={2019},
  volume={65},
  number={2},
  pages={188-194},
  doi={10.1109/TCE.2019.2899067}}